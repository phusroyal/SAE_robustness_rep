{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83addd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"https_proxy\"] = \"http://xen03.iitd.ac.in:3128\"\n",
    "os.environ[\"http_proxy\"] = \"http://xen03.iitd.ac.in:3128\"\n",
    "\n",
    "import sys\n",
    "# sys.path.append('../sae')\n",
    "from sae import Sae\n",
    "from utils import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a5004c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0a8b4c425046dcaedab2217093ca04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import model\n",
    "model_type = 'gemma2-2b'\n",
    "layer_num = 20\n",
    "device = 'cpu'\n",
    "model, tokenizer, sae = load_model_and_sae(model_type, layer_num, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062a7ff",
   "metadata": {},
   "source": [
    "# Interpretation and Conclusions\n",
    "print(\"=== BOUNDARY CROSSING ANALYSIS CONCLUSIONS ===\\n\")\n",
    "\n",
    "if 'batch_results' in locals() and batch_results:\n",
    "    successful_results = {k: v for k, v in batch_results.items() if v.target_reached}\n",
    "    \n",
    "    if successful_results:\n",
    "        delta_norms = [r.delta_min for r in successful_results.values()]\n",
    "        mean_boundary_dist = np.mean(delta_norms)\n",
    "        std_boundary_dist = np.std(delta_norms)\n",
    "        \n",
    "        print(\"1. MARGIN ANALYSIS:\")\n",
    "        print(f\"   - Average minimal perturbation to cross boundary: {mean_boundary_dist:.6f}\")\n",
    "        print(f\"   - Standard deviation: {std_boundary_dist:.6f}\")\n",
    "        print(f\"   - This suggests SAE boundaries are {'close' if mean_boundary_dist < 0.1 else 'distant'}\")\n",
    "        \n",
    "        # Compare with typical input norms for context\n",
    "        if 'all_texts' in locals():\n",
    "            hidden_norms = []\n",
    "            for text in all_texts[:3]:  # Sample a few to get typical norms\n",
    "                h = tracer.get_hidden_representation(text, layer_num)\n",
    "                hidden_norms.append(h.norm().item())\n",
    "            avg_hidden_norm = np.mean(hidden_norms)\n",
    "            relative_perturbation = mean_boundary_dist / avg_hidden_norm\n",
    "            print(f\"   - Relative to input magnitude ({avg_hidden_norm:.2f}): {relative_perturbation:.4f} ({relative_perturbation*100:.2f}%)\")\n",
    "\n",
    "if 'concept_results' in locals() and concept_results:\n",
    "    print(\"\\n2. CONCEPT DISTANCE ANALYSIS:\")\n",
    "    \n",
    "    concept_distances = []\n",
    "    boundary_distances = []\n",
    "    ratios = []\n",
    "    \n",
    "    for data in concept_results.values():\n",
    "        concept_distances.extend([data['a_to_b_distance'], data['b_to_a_distance']])\n",
    "        boundary_distances.extend([data['a_boundary_distance'], data['b_boundary_distance']])\n",
    "        if data['distance_ratio_a'] != float('inf'):\n",
    "            ratios.append(data['distance_ratio_a'])\n",
    "        if data['distance_ratio_b'] != float('inf'):\n",
    "            ratios.append(data['distance_ratio_b'])\n",
    "    \n",
    "    if ratios:\n",
    "        mean_ratio = np.mean(ratios)\n",
    "        print(f\"   - Concept distances are {mean_ratio:.1f}x larger than single boundary distances\")\n",
    "        print(f\"   - This means crossing from Education→Technology requires crossing ~{mean_ratio:.0f} boundaries\")\n",
    "        \n",
    "        if mean_ratio > 3:\n",
    "            print(\"   - INTERPRETATION: Concepts are well-separated in SAE space\")\n",
    "        elif mean_ratio > 1.5:\n",
    "            print(\"   - INTERPRETATION: Moderate concept separation\")\n",
    "        else:\n",
    "            print(\"   - INTERPRETATION: Concepts may be close together (potential vulnerability)\")\n",
    "\n",
    "if 'feature_flips' in locals() and feature_flips:\n",
    "    print(\"\\n3. FEATURE VULNERABILITY:\")\n",
    "    vulnerable_features = len([f for f, counts in feature_flips.items() if counts['total'] >= 2])\n",
    "    total_features = len(original_code)  # Assuming this is available\n",
    "    vulnerability_rate = vulnerable_features / total_features if total_features > 0 else 0\n",
    "    \n",
    "    print(f\"   - {vulnerable_features} out of {total_features} features are consistently vulnerable\")\n",
    "    print(f\"   - Vulnerability rate: {vulnerability_rate:.3f} ({vulnerability_rate*100:.1f}%)\")\n",
    "    \n",
    "    if vulnerability_rate > 0.1:\n",
    "        print(\"   - INTERPRETATION: High feature vulnerability - SAE may be susceptible to attacks\")\n",
    "    elif vulnerability_rate > 0.05:\n",
    "        print(\"   - INTERPRETATION: Moderate vulnerability\")\n",
    "    else:\n",
    "        print(\"   - INTERPRETATION: Low vulnerability - SAE appears robust\")\n",
    "\n",
    "print(\"\\n4. ROBUSTNESS ASSESSMENT:\")\n",
    "if 'successful_results' in locals() and len(successful_results) > 0:\n",
    "    success_rate = len(successful_results) / len(batch_results) if batch_results else 0\n",
    "    print(f\"   - Boundary search success rate: {success_rate:.2f} ({success_rate*100:.1f}%)\")\n",
    "    \n",
    "    if success_rate > 0.8:\n",
    "        print(\"   - INTERPRETATION: Boundaries are easily found - potential robustness concerns\")\n",
    "    elif success_rate > 0.5:\n",
    "        print(\"   - INTERPRETATION: Moderate boundary accessibility\")\n",
    "    else:\n",
    "        print(\"   - INTERPRETATION: Boundaries are hard to find - suggests robustness\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(\"1. Consider adversarial training to increase boundary distances\")\n",
    "print(\"2. Monitor vulnerable features during deployment\")\n",
    "print(\"3. Test with more diverse text pairs to validate generalization\")\n",
    "print(\"4. Compare with other SAE architectures for robustness benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis: Which SAE features are most vulnerable to boundary crossings?\n",
    "# This helps identify which features are closest to decision boundaries\n",
    "\n",
    "if 'batch_results' in locals() and batch_results:\n",
    "    print(\"Analyzing feature vulnerability...\")\n",
    "    \n",
    "    # Collect all flipped features across all successful boundary crossings\n",
    "    feature_flips = {}\n",
    "    for result in batch_results.values():\n",
    "        if result.target_reached:\n",
    "            for feature_idx, flip_type in result.flipped_features:\n",
    "                if feature_idx not in feature_flips:\n",
    "                    feature_flips[feature_idx] = {'activate': 0, 'deactivate': 0, 'total': 0}\n",
    "                feature_flips[feature_idx][flip_type] += 1\n",
    "                feature_flips[feature_idx]['total'] += 1\n",
    "    \n",
    "    if feature_flips:\n",
    "        # Sort features by total flip frequency\n",
    "        sorted_features = sorted(feature_flips.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nMost Vulnerable Features (top 10):\")\n",
    "        print(\"Feature ID | Total Flips | Activations | Deactivations\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        for i, (feature_idx, counts) in enumerate(sorted_features[:10]):\n",
    "            print(f\"{feature_idx:9d} | {counts['total']:11d} | {counts['activate']:11d} | {counts['deactivate']:13d}\")\n",
    "        \n",
    "        # Analyze flip patterns\n",
    "        total_features_flipped = len(feature_flips)\n",
    "        total_flips = sum(counts['total'] for counts in feature_flips.values())\n",
    "        \n",
    "        print(f\"\\nFeature Vulnerability Summary:\")\n",
    "        print(f\"- Total unique features that flipped: {total_features_flipped}\")\n",
    "        print(f\"- Total feature flips across all texts: {total_flips}\")\n",
    "        print(f\"- Average flips per vulnerable feature: {total_flips/total_features_flipped:.2f}\")\n",
    "        \n",
    "        # Check if some features are consistently more vulnerable\n",
    "        frequent_flippers = [f for f, counts in feature_flips.items() if counts['total'] >= 2]\n",
    "        print(f\"- Features that flipped multiple times: {len(frequent_flippers)}\")\n",
    "        \n",
    "        if frequent_flippers:\n",
    "            print(f\"- Most vulnerable feature: {sorted_features[0][0]} (flipped {sorted_features[0][1]['total']} times)\")\n",
    "    else:\n",
    "        print(\"No feature flips found in the results.\")\n",
    "else:\n",
    "    print(\"No batch results available for feature vulnerability analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the boundary analysis results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Visualize batch boundary analysis\n",
    "if 'batch_results' in locals() and batch_results:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Boundary distance distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    successful_results = {k: v for k, v in batch_results.items() if v.target_reached}\n",
    "    if successful_results:\n",
    "        delta_norms = [r.delta_min for r in successful_results.values()]\n",
    "        plt.hist(delta_norms, bins=10, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Minimal Perturbation Norm ||δ_min||')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Boundary Distances')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature flips distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if successful_results:\n",
    "        num_flips = [len(r.flipped_features) for r in successful_results.values()]\n",
    "        if num_flips and max(num_flips) > 0:\n",
    "            plt.hist(num_flips, bins=max(num_flips)+1, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Number of Feature Flips')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Feature Flips at Boundaries')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Success rate\n",
    "    plt.subplot(1, 3, 3)\n",
    "    total_texts = len(batch_results)\n",
    "    successful_texts = len(successful_results)\n",
    "    labels = ['Successful', 'Failed']\n",
    "    sizes = [successful_texts, total_texts - successful_texts]\n",
    "    colors = ['lightgreen', 'lightcoral']\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Boundary Search Success Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize concept distances if available\n",
    "if 'concept_results' in locals() and concept_results:\n",
    "    visualize_concept_distances(concept_results, save_path='concept_distances.png')\n",
    "\n",
    "print(\"Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept Distance Analysis: Education vs Technology\n",
    "# Measure how many boundaries need to be crossed to go from one concept to another\n",
    "\n",
    "concept_pairs = [\n",
    "    (education_texts[0], technology_texts[0]),\n",
    "    (education_texts[1], technology_texts[1]),\n",
    "    (education_texts[2], technology_texts[2])\n",
    "]\n",
    "\n",
    "print(\"Analyzing concept distances between education and technology...\")\n",
    "print(f\"Number of concept pairs: {len(concept_pairs)}\")\n",
    "\n",
    "# Run concept distance analysis\n",
    "concept_results = tracer.concept_distance_analysis(\n",
    "    concept_pairs,\n",
    "    layer_idx=layer_num,\n",
    "    max_iterations=100  # More iterations for targeted search\n",
    ")\n",
    "\n",
    "print(f\"\\nConcept Distance Results:\")\n",
    "for pair_key, data in concept_results.items():\n",
    "    print(f\"\\n{pair_key}:\")\n",
    "    print(f\"  Education → Technology: {data['a_to_b_distance']:.6f} (reached: {data['a_to_b_reached']})\")\n",
    "    print(f\"  Technology → Education: {data['b_to_a_distance']:.6f} (reached: {data['b_to_a_reached']})\")\n",
    "    print(f\"  Education boundary distance: {data['a_boundary_distance']:.6f}\")\n",
    "    print(f\"  Technology boundary distance: {data['b_boundary_distance']:.6f}\")\n",
    "    \n",
    "    if data['distance_ratio_a'] != float('inf'):\n",
    "        print(f\"  Education concept distance ratio: {data['distance_ratio_a']:.2f}x single boundary\")\n",
    "    if data['distance_ratio_b'] != float('inf'):\n",
    "        print(f\"  Technology concept distance ratio: {data['distance_ratio_b']:.2f}x single boundary\")\n",
    "\n",
    "# Summary statistics\n",
    "all_concept_distances = []\n",
    "all_boundary_distances = []\n",
    "all_ratios = []\n",
    "\n",
    "for data in concept_results.values():\n",
    "    all_concept_distances.extend([data['a_to_b_distance'], data['b_to_a_distance']])\n",
    "    all_boundary_distances.extend([data['a_boundary_distance'], data['b_boundary_distance']])\n",
    "    if data['distance_ratio_a'] != float('inf'):\n",
    "        all_ratios.append(data['distance_ratio_a'])\n",
    "    if data['distance_ratio_b'] != float('inf'):\n",
    "        all_ratios.append(data['distance_ratio_b'])\n",
    "\n",
    "print(f\"\\nOverall Summary:\")\n",
    "print(f\"- Mean concept distance: {np.mean(all_concept_distances):.6f}\")\n",
    "print(f\"- Mean single boundary distance: {np.mean(all_boundary_distances):.6f}\")\n",
    "if all_ratios:\n",
    "    print(f\"- Mean distance ratio: {np.mean(all_ratios):.2f}x\")\n",
    "    print(f\"- This suggests concepts are {np.mean(all_ratios):.1f}x farther apart than single boundaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed21dc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'education_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Batch analysis for multiple texts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m \u001b[43meducation_texts\u001b[49m \u001b[38;5;241m+\u001b[39m technology_texts\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m texts for boundary distances...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run batch analysis (this might take a while)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'education_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch analysis for multiple texts\n",
    "all_texts = education_texts + technology_texts\n",
    "\n",
    "print(f\"Analyzing {len(all_texts)} texts for boundary distances...\")\n",
    "\n",
    "# Run batch analysis (this might take a while)\n",
    "batch_results = tracer.analyze_margins_batch(\n",
    "    all_texts, \n",
    "    layer_idx=layer_num,\n",
    "    max_iterations=50,  # Reduced for faster testing\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBatch Analysis Summary:\")\n",
    "print(f\"- Total texts processed: {len(batch_results)}\")\n",
    "\n",
    "# Analyze the results\n",
    "successful_results = {k: v for k, v in batch_results.items() if v.target_reached}\n",
    "print(f\"- Successful boundary crossings: {len(successful_results)}\")\n",
    "\n",
    "if successful_results:\n",
    "    delta_norms = [r.delta_min for r in successful_results.values()]\n",
    "    print(f\"- Mean boundary distance: {np.mean(delta_norms):.6f}\")\n",
    "    print(f\"- Std boundary distance: {np.std(delta_norms):.6f}\")\n",
    "    print(f\"- Min boundary distance: {np.min(delta_norms):.6f}\")\n",
    "    print(f\"- Max boundary distance: {np.max(delta_norms):.6f}\")\n",
    "    \n",
    "    # Count feature flips\n",
    "    all_flips = [len(r.flipped_features) for r in successful_results.values()]\n",
    "    print(f\"- Mean feature flips: {np.mean(all_flips):.2f}\")\n",
    "    print(f\"- Total unique flipped features: {len(set(f for r in successful_results.values() for f, _ in r.flipped_features))}\")\n",
    "else:\n",
    "    print(\"- No successful boundary crossings found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47860c",
   "metadata": {},
   "source": [
    "# Experiment 1: Measuring Margins and Boundary Crossings\n",
    "# Find minimal perturbations that cause SAE code changes\n",
    "\n",
    "# Test with a single example first\n",
    "test_text = education_texts[0]\n",
    "print(f\"Testing with: '{test_text}'\")\n",
    "\n",
    "# Extract hidden representation\n",
    "hidden_state = tracer.get_hidden_representation(test_text, layer_num)\n",
    "print(f\"Hidden state shape: {hidden_state.shape}\")\n",
    "\n",
    "# Get original SAE code\n",
    "original_code = tracer.get_sae_code(hidden_state)\n",
    "original_active = (original_code > 0).sum().item()\n",
    "print(f\"Original code has {original_active} active features out of {len(original_code)}\")\n",
    "\n",
    "# Find minimal boundary perturbation\n",
    "result = tracer.find_minimal_boundary_perturbation(\n",
    "    hidden_state, \n",
    "    max_iterations=50,  # Start with fewer iterations for testing\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBoundary Analysis Results:\")\n",
    "print(f\"- Minimal perturbation norm: {result.delta_min:.6f}\")\n",
    "print(f\"- Boundary reached: {result.target_reached}\")\n",
    "print(f\"- Number of steps: {result.num_steps}\")\n",
    "print(f\"- Features flipped: {result.flipped_features}\")\n",
    "print(f\"- Boundary type: {result.boundary_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ec85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between h1_raw and h2_raw: 0.8860180974006653\n",
      "Cosine similarity between h1_raw and h3_raw: 0.8103485703468323\n",
      "Cosine similarity between h2_raw and h3_raw: 0.9335434436798096\n",
      "------------------------------------------------------------\n",
      "Cosine similarity between h1_raw and base_raw: 0.860113263130188\n",
      "Cosine similarity between h2_raw and base_raw: 0.8216165900230408\n",
      "Cosine similarity between h3_raw and base_raw: 0.788619875907898\n"
     ]
    }
   ],
   "source": [
    "# Example: Boundary Crossing Analysis for SAE Robustness\n",
    "# This demonstrates how to use the boundary_crossing module\n",
    "\n",
    "from boundary_crossing import SAEBoundaryTracer, visualize_boundary_analysis, visualize_concept_distances\n",
    "\n",
    "# Initialize the boundary tracer\n",
    "tracer = SAEBoundaryTracer(sae, tokenizer, model, device=device)\n",
    "\n",
    "# Example texts from Li et al. study\n",
    "education_texts = [\n",
    "    \"The university offers comprehensive programs in computer science and engineering.\",\n",
    "    \"Students can pursue advanced degrees in mathematics and physics.\",\n",
    "    \"The curriculum includes hands-on laboratory experiences and research opportunities.\"\n",
    "]\n",
    "\n",
    "technology_texts = [\n",
    "    \"The new smartphone features advanced AI capabilities and improved battery life.\",\n",
    "    \"Cloud computing platforms enable scalable and efficient data processing.\",\n",
    "    \"Machine learning algorithms are revolutionizing industries across the globe.\"\n",
    "]\n",
    "\n",
    "print(\"Boundary crossing analysis setup complete!\")\n",
    "print(f\"Education texts: {len(education_texts)}\")\n",
    "print(f\"Technology texts: {len(technology_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "577801e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of h1_plt: torch.Size([2304])\n",
      "Hamming distance between h1 and h2: 595\n",
      "Hamming distance between h1 and h3: 743\n",
      "Hamming distance between h2 and h3: 412\n",
      "Hamming distance between h1 and base: 643\n",
      "Hamming distance between h2 and base: 744\n",
      "Hamming distance between h3 and base: 806\n"
     ]
    }
   ],
   "source": [
    "def polytope(x):\n",
    "    \"\"\"\n",
    "    If value is greater than 0, then 1 else 0\n",
    "    \"\"\"\n",
    "    return (x > 0).float()\n",
    "\n",
    "h1_plt = polytope(h1_raw)\n",
    "h2_plt = polytope(h2_raw)\n",
    "h3_plt = polytope(h3_raw)\n",
    "base_plt = polytope(h_base_raw)\n",
    "\n",
    "# Print the shapes of the polytopes\n",
    "print(f\"Shape of h1_plt: {h1_plt.shape}\")\n",
    "\n",
    "# hamming distance\n",
    "hamming_distance = torch.sum(h1_plt != h2_plt).item()\n",
    "print(f\"Hamming distance between h1 and h2: {hamming_distance}\")\n",
    "\n",
    "hamming_distance = torch.sum(h1_plt != h3_plt).item()\n",
    "print(f\"Hamming distance between h1 and h3: {hamming_distance}\")\n",
    "\n",
    "hamming_distance = torch.sum(h2_plt != h3_plt).item()\n",
    "print(f\"Hamming distance between h2 and h3: {hamming_distance}\")\n",
    "\n",
    "hamming_distance = torch.sum(h1_plt != base_plt).item()\n",
    "print(f\"Hamming distance between h1 and base: {hamming_distance}\")\n",
    "\n",
    "hamming_distance = torch.sum(h2_plt != base_plt).item()\n",
    "print(f\"Hamming distance between h2 and base: {hamming_distance}\")\n",
    "\n",
    "hamming_distance = torch.sum(h3_plt != base_plt).item()\n",
    "print(f\"Hamming distance between h3 and base: {hamming_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5dc3c4",
   "metadata": {},
   "source": [
    "Extract sae features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f466a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=170\n",
    "\n",
    "z1,s1,s1_acts = extract_sae_features(h1_raw, sae, model_type, k)\n",
    "z2,s2,s2_acts = extract_sae_features(h2_raw, sae, model_type, k)\n",
    "z3,s3,s3_acts = extract_sae_features(h3_raw, sae, model_type, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37194ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming distance between z1 and z2: 229\n",
      "Hamming distance between z1 and z3: 234\n",
      "Hamming distance between z2 and z3: 157\n"
     ]
    }
   ],
   "source": [
    "# value in z1, z2, z3 are not 1 only\n",
    "# appy polytope\n",
    "z1_plt = polytope(z1)\n",
    "z2_plt = polytope(z2)\n",
    "z3_plt = polytope(z3)\n",
    "\n",
    "# hamming distance\n",
    "hamming_distance = torch.sum(z1_plt != z2_plt).item()\n",
    "print(f\"Hamming distance between z1 and z2: {hamming_distance}\")\n",
    "hamming_distance = torch.sum(z1_plt != z3_plt).item()  \n",
    "print(f\"Hamming distance between z1 and z3: {hamming_distance}\")\n",
    "hamming_distance = torch.sum(z2_plt != z3_plt).item()\n",
    "print(f\"Hamming distance between z2 and z3: {hamming_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap score between s1 and s2: 0.4000000059604645\n",
      "Overlap score between s1 and s3: 0.3529411852359772\n",
      "Overlap score between s2 and s3: 0.6235294342041016\n"
     ]
    }
   ],
   "source": [
    "# how about get overlap\n",
    "overlap_score = get_overlap(s1, s2)\n",
    "print(f\"Overlap score between s1 and s2: {overlap_score}\")\n",
    "overlap_score = get_overlap(s1, s3)\n",
    "print(f\"Overlap score between s1 and s3: {overlap_score}\")\n",
    "overlap_score = get_overlap(s2, s3)\n",
    "print(f\"Overlap score between s2 and s3: {overlap_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34cef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c3aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735c3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
